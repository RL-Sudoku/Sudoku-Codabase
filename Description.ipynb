{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2eca81c",
   "metadata": {},
   "source": [
    "This scope of our proejct is to replicate the game of Uno in a way to allow for training with reinforcement learning type techniques. In order to make this feasible, we had to make some simplifications to the game as a way to better represent state space. For example, we decided not to include the color of the special cards (e.g. skip, reverse, etc). Despite this, Uno is still a very stochastic game, and thus this modification will yield a state space that is on the scale of $10^{72}$. This, by no means, can be represented tabuluarly, and thus we have opted to use a neural network with 2 hidden layers. (This justification is provided to us by In addition, one of the key assumptions we are making is that the game must end (as there are no ties), and the deck we are using is endless, meaning that we have a countless number of the all card. This ensures do not have to worry about determining the conditional probability of a card, relative to the previous cards we've already seen. By using this setup, we then trained an agent using the function approximation version of the following algorithms: Monte Carlo, REINFORCE, SARSA, and Double QN. For some of these algorithms, we have slightly modifed them in a way (i.e., storing the values of state-action pairs) so that we can compare the efficacy of the algorithms on the same metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1326d576",
   "metadata": {},
   "source": [
    "Each person in the group was tasked to train an agent with one of algorithms listed above. The breakdown is as follows:\n",
    "\n",
    "    - Fiorella Chen: Monte Carlo\n",
    "    - Jules Deschamps: REINFORCE\n",
    "    - Xiaoyang Song: SARSA\n",
    "    - Savannah Tan: Double QN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30961f95",
   "metadata": {},
   "source": [
    "In terms of setting up the python environment for uno, we heavily relied on the rlcard package in python, and we have modified and overwrote certain files to make it compatible for training. After training against a fully random player (which was avaible to us through rlcard), we fine-tuned the hyperparameters for each trained agent in order to maximize the total winning rate and later played the agents against each other to determine which agent had best learned Uno. The results are stored in a 5 by 5 table—4 training agents and the random agent—where the row player plays first and the column player plays second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aef8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
