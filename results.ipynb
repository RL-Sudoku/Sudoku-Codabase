{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"./\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### State, Action & Reward Representation\n",
    "\n",
    "- We only consider 1v1 UNO\n",
    "\n",
    "- 2 states representations: naive representation for tabular methods (where you count all possibilities) results in way too large a state space to run tabular methods (if you do not even account for values this results in ~130M possible states). Needs reducing the state space through simplification --> 4 planes where each entry of a plane corresponds to a given card and 3 of those planes to having a given number of a specific card (you can either have 0, 1, or 2 of each card) ; the last plane is the target / open card.\n",
    "\n",
    "- Motivations of using function approximation: state space is way too big.\n",
    "\n",
    "- In total, there are 61 different actions a player can take, which are playing each of the 60 different cards and drawing a card from the deck.\n",
    "\n",
    "- The reward is defined as +1 for winning, −1 for losing, and 0 for all intermediate states. Also we assume the game is endless so all final rewards are either +1 or -1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture & Hyperparameter Selection\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Add plot later\n",
    "\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "Add description later"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents & Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Random Policy\n",
    "Random vs. Random: we observe that the advantage given by starting the game is not negligible (the random player that starts has an average reward of .02 if I remember). This is not surprising though: in a game where the goal is to let go of your cards as fast as possible, getting rid of cards in a first place will result in higher rewrad in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: 0.0084\n",
      "RANDOM Agent Average Reward: -0.0084\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 5042 games (RANDOM Agent win rate: 50.42%)\n",
      "RANDOM Agent wins 4958 games (RANDOM Agent win rate: 49.58%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0084, -0.0084)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tests.eval import test_trained_agents\n",
    "from uno.agents.random_agent import RandomAgent\n",
    "random_agent = RandomAgent(61)\n",
    "test_trained_agents(random_agent, random_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm I: REINFORCE\n",
    "\n",
    "TODO: Change this plot later\n",
    "\n",
    "TODO: Add description\n",
    "<!-- Average Rewards\n",
    "\n",
    "------------------------------------------------------------\n",
    "Reinforce Agent Average Reward: 0.0038\n",
    "RANDOM Agent Average Reward: -0.0038\n",
    "\n",
    "------------------------------------------------------------\n",
    "Total Number of Games: 10000\n",
    "Reinforce Agent wins 5019 games (Reinforce Agent win rate: 50.19%)\n",
    "RANDOM Agent wins 4981 games (RANDOM Agent win rate: 49.81%)\n",
    "Draws 0 games (Draw rate: 0.0%) -->\n",
    "\n",
    "<!-- Very inconclusive -->\n",
    "![avg_reward_reinforce.png](checkpoint/REINFORCE/avg_reward_reinforce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: -0.0212\n",
      "REINFORCE Agent Average Reward: 0.0212\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 4894 games (RANDOM Agent win rate: 48.94%)\n",
      "REINFORCE Agent wins 5106 games (REINFORCE Agent win rate: 51.06%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.reinforce_agent import ReinforceAgent\n",
    "reinforce_agent = CHECKPOINTS['REINFORCE Agent']\n",
    "rewards = test_trained_agents(random_agent, reinforce_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "REINFORCE Agent Average Reward: 0.0132\n",
      "RANDOM Agent Average Reward: -0.0132\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "REINFORCE Agent wins 5066 games (REINFORCE Agent win rate: 50.66%)\n",
      "RANDOM Agent wins 4934 games (RANDOM Agent win rate: 49.34%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "rewards = test_trained_agents(reinforce_agent, random_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm II: Monte Carlo On Policy Approximation\n",
    "\n",
    "TODO: Add description\n",
    "\n",
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/MC/mc-agent-[200000]-[0.0001]-[0.95]-[0.95]-[first].png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/MC/mc-agent-[200000]-[0.0001]-[0.95]-[0.95]-[first].png\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: -0.0258\n",
      "MC Agent Average Reward: 0.0258\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 4871 games (RANDOM Agent win rate: 48.71%)\n",
      "MC Agent wins 5129 games (MC Agent win rate: 51.29%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.mc_agent import MCAgent\n",
    "mc_agent = CHECKPOINTS['MC Agent']\n",
    "rewards = test_trained_agents(random_agent, mc_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "MC Agent Average Reward: 0.0384\n",
      "RANDOM Agent Average Reward: -0.0384\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "MC Agent wins 5192 games (MC Agent win rate: 51.92%)\n",
      "RANDOM Agent wins 4808 games (RANDOM Agent win rate: 48.08%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "rewards = test_trained_agents(mc_agent, random_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm III: Double Q-Learning\n",
    "\n",
    "TODO: Add more descriptions\n",
    "\n",
    "#### Double-Q Network\n",
    "\n",
    "The Double Deep Q-Network (Double DQN) is an extension of the standard Deep Q-Network (DQN) that aims to address the overestimation bias often found in Q-learning algorithms. In traditional DQN, the same network is used to both select and evaluate the best action, which can lead to an overoptimistic estimation of action values. To mitigate this issue, the Double DQN employs two separate networks: one for action selection and another for action evaluation. During each iteration, the agent plays n games using ε-greedy exploration and generates a set of trajectories (s, a, r, s'). The Double DQN update rule then utilizes both networks by selecting the action with the highest Q-value from the first network and evaluating that action using the second network. This decouples the action selection and evaluation processes, reducing the overestimation bias and improving the stability of learning.\n",
    "\n",
    "dqn agent was trained with a set of different parameters:\n",
    "Number of episodes: 100k, 80k, 50k,\n",
    "decaying epsilon from 0.95 to 0.01 with decaying factor 0.95, updating every 1000 episodes.\n",
    "Constant epsilon: 0.05,0.5,0.8.\n",
    "\n",
    "\n",
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/DQN/dqn-agent-[200000]-[0.0001]-[no decay_0.08]-[0.95]-[first].png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/DQN/dqn-agent-[200000]-[0.0001]-[no decay_0.08]-[0.95]-[second].png\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Unfortunately, we are only using CPUs now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional or an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32mmtrand.pyx:905\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiaoyangsong/Desktop/Reinforcement Learning/UNO-Codabase/results.ipynb Cell 15\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/Reinforcement%20Learning/UNO-Codabase/results.ipynb#X32sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m random_agent \u001b[39m=\u001b[39m RandomAgent(\u001b[39m61\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/Reinforcement%20Learning/UNO-Codabase/results.ipynb#X32sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dqn_agent \u001b[39m=\u001b[39m CHECKPOINTS[\u001b[39m'\u001b[39m\u001b[39mDQN Agent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/Reinforcement%20Learning/UNO-Codabase/results.ipynb#X32sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m rewards \u001b[39m=\u001b[39m test_trained_agents(dqn_agent, random_agent, \u001b[39m10000\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Desktop/Reinforcement Learning/UNO-Codabase/tests/eval.py:26\u001b[0m, in \u001b[0;36mtest_trained_agents\u001b[0;34m(agent1, agent2, n, verbose)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n), disable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m     env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m---> 26\u001b[0m     trajectories, payoffs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     27\u001b[0m     payoffs_lst\u001b[39m.\u001b[39mappend(payoffs)\n\u001b[1;32m     28\u001b[0m     trajectories_lst\u001b[39m.\u001b[39mappend(trajectories)\n",
      "File \u001b[0;32m~/Desktop/Reinforcement Learning/UNO-Codabase/uno/envs/env.py:144\u001b[0m, in \u001b[0;36mEnv.run\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    141\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents[player_id]\u001b[39m.\u001b[39mstep(state)\n\u001b[1;32m    143\u001b[0m \u001b[39m# Environment steps\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m next_state, next_player_id \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m    145\u001b[0m     action, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magents[player_id]\u001b[39m.\u001b[39;49muse_raw)\n\u001b[1;32m    146\u001b[0m \u001b[39m# Save action\u001b[39;00m\n\u001b[1;32m    147\u001b[0m trajectories[player_id]\u001b[39m.\u001b[39mappend(action)\n",
      "File \u001b[0;32m~/Desktop/Reinforcement Learning/UNO-Codabase/uno/envs/env.py:71\u001b[0m, in \u001b[0;36mEnv.step\u001b[0;34m(self, action, raw_action)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39m''' Step forward\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m        (int): The ID of the next player\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_action:\n\u001b[0;32m---> 71\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode_action(action)\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimestep \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     74\u001b[0m \u001b[39m# Record the action for human interface\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Reinforcement Learning/UNO-Codabase/uno/envs/unoenv.py:48\u001b[0m, in \u001b[0;36mUnoEnv._decode_action\u001b[0;34m(self, action_id)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mif\u001b[39;00m action_id \u001b[39min\u001b[39;00m legal_ids:\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m ACTION_LIST[action_id]\n\u001b[0;32m---> 48\u001b[0m \u001b[39mreturn\u001b[39;00m ACTION_LIST[np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(legal_ids)]\n",
      "File \u001b[0;32mmtrand.pyx:907\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be 1-dimensional or an integer"
     ]
    }
   ],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.dqn_agent import DQNAgent\n",
    "from tests.eval import test_trained_agents\n",
    "from uno.agents.random_agent import RandomAgent\n",
    "random_agent = RandomAgent(61)\n",
    "dqn_agent = CHECKPOINTS['DQN Agent']\n",
    "rewards = test_trained_agents(dqn_agent, random_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "DQN Agent Average Reward: 0.029\n",
      "RANDOM Agent Average Reward: -0.029\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "DQN Agent wins 5145 games (DQN Agent win rate: 51.45%)\n",
      "RANDOM Agent wins 4855 games (RANDOM Agent win rate: 48.55%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.dqn_agent import DQNAgent\n",
    "dqn_agent = CHECKPOINTS['DQN Agent']\n",
    "rewards = test_trained_agents(dqn_agent, random_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: -0.0148\n",
      "DQN Agent Average Reward: 0.0148\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 4926 games (RANDOM Agent win rate: 49.26%)\n",
      "DQN Agent wins 5074 games (DQN Agent win rate: 50.74%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "rewards = test_trained_agents(random_agent, dqn_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm IV: SARSA\n",
    "\n",
    "TODO: Add description\n",
    "\n",
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/SARSA/sarsa-agent-[200000]-[0.0001]-[0.95]-[0.95]-[first]-[0].png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/SARSA/sarsa-agent-[200000]-[0.0001]-[0.95]-[0.95]-[second]-[0].png\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:42<00:00, 97.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: -0.1204\n",
      "SARSA Agent Average Reward: 0.1204\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 4398 games (RANDOM Agent win rate: 43.98%)\n",
      "SARSA Agent wins 5602 games (SARSA Agent win rate: 56.02%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "from tests.tests import *\n",
    "from uno.agents.sarsa_agent import SARSAAgent\n",
    "sarsa_agent = CHECKPOINTS['SARSA Agent']\n",
    "# sarsa_agent = torch.load(\n",
    "#         \"checkpoint/SARSA/best_agent.pt\", map_location=DEVICE)\n",
    "# sarsa_agent.eps = 0.01\n",
    "# # For testing purpose only (remove this line later)\n",
    "# setattr(sarsa_agent, \"name\", \"SARSA Agent\")\n",
    "rewards = test_trained_agents(random_agent, sarsa_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:26<00:00, 115.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "SARSA Agent Average Reward: 0.141\n",
      "RANDOM Agent Average Reward: -0.141\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "SARSA Agent wins 5705 games (SARSA Agent win rate: 57.05%)\n",
      "RANDOM Agent wins 4295 games (RANDOM Agent win rate: 42.95%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2023)\n",
    "from tests.tests import *\n",
    "rewards = test_trained_agents(sarsa_agent, random_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contests\n",
    "\n",
    "In this section, we play against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 195.59it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 90.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 106.89it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 92.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 64.29it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 81.29it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 44.66it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 67.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 54.68it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 45.65it/s]\n",
      " 30%|███       | 3/10 [00:00<00:00, 36.11it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/xiaoyangsong/Desktop/Reinforcement Learning/UNO-Codabase/results.ipynb Cell 22\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/Reinforcement%20Learning/UNO-Codabase/results.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtests\u001b[39;00m \u001b[39mimport\u001b[39;00m contests\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/Reinforcement%20Learning/UNO-Codabase/results.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtests\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meval\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/Reinforcement%20Learning/UNO-Codabase/results.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m stats \u001b[39m=\u001b[39m contests(n\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/xiaoyangsong/Desktop/Reinforcement%20Learning/UNO-Codabase/results.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m stats\n",
      "File \u001b[0;32m~/Desktop/Reinforcement Learning/UNO-Codabase/tests/tests.py:50\u001b[0m, in \u001b[0;36mcontests\u001b[0;34m(n, seed)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39msetattr\u001b[39m(agent2_ckpt, \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m, agent2)\n\u001b[1;32m     47\u001b[0m \u001b[39m# Test\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m# print(agent1)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m# print(agent2)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m r, _ \u001b[39m=\u001b[39m test_trained_agents(\n\u001b[1;32m     51\u001b[0m     agent1_ckpt, agent2_ckpt, n, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     52\u001b[0m \u001b[39m# ic(r)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m agent1_win_rate \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m+\u001b[39m r \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Reinforcement Learning/UNO-Codabase/tests/eval.py:26\u001b[0m, in \u001b[0;36mtest_trained_agents\u001b[0;34m(agent1, agent2, n, verbose)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(n), disable\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     25\u001b[0m     env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m---> 26\u001b[0m     trajectories, payoffs \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m     27\u001b[0m     payoffs_lst\u001b[39m.\u001b[39mappend(payoffs)\n\u001b[1;32m     28\u001b[0m     trajectories_lst\u001b[39m.\u001b[39mappend(trajectories)\n",
      "File \u001b[0;32m~/Desktop/Reinforcement Learning/UNO-Codabase/uno/envs/env.py:139\u001b[0m, in \u001b[0;36mEnv.run\u001b[0;34m(self, is_training)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_over():\n\u001b[1;32m    135\u001b[0m     \u001b[39m# Agent plays\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_training:\n\u001b[1;32m    137\u001b[0m         \u001b[39m# ic(player_id)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         \u001b[39m# ic(state)\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m         action, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magents[player_id]\u001b[39m.\u001b[39;49meval_step(state)\n\u001b[1;32m    140\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents[player_id]\u001b[39m.\u001b[39mstep(state)\n",
      "File \u001b[0;32m~/Desktop/Reinforcement Learning/UNO-Codabase/uno/agents/sarsa_agent.py:60\u001b[0m, in \u001b[0;36mSARSAAgent.eval_step\u001b[0;34m(self, state, is_greedy)\u001b[0m\n\u001b[1;32m     58\u001b[0m legal_actions \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(state[\u001b[39m'\u001b[39m\u001b[39mlegal_actions\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m     59\u001b[0m \u001b[39m# Obtain action values by approximation\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m val_lst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mQ(state[\u001b[39m'\u001b[39;49m\u001b[39mobs\u001b[39;49m\u001b[39m'\u001b[39;49m])[legal_actions]\n\u001b[1;32m     61\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(legal_actions) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(val_lst)\n\u001b[1;32m     62\u001b[0m \u001b[39m# Action\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Reinforcement Learning/UNO-Codabase/model/sarsa_backbone.py:27\u001b[0m, in \u001b[0;36mSARSA_Q.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(state) \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m     25\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m     26\u001b[0m         state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 27\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(torch\u001b[39m.\u001b[39;49mflatten(state\u001b[39m.\u001b[39;49mto(DEVICE), \u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[1;32m     28\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "from tests.tests import contests\n",
    "from tests.eval import *\n",
    "stats = contests(n=1000)\n",
    "stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Limitation\n",
    "\n",
    "#### Hyperparameters & Architecture\n",
    "\n",
    "A majority of the agents we trained do not seem to have any edge over the random agent, which sounds very frustrating considering UNO does not look that complicated after all. A lot of parameters come into play regarding the performance of the agents we propose here however, namely the architecture of our NNs and the hyperparameters.\n",
    "\n",
    "Most of our fine-tuning is heavily inspired by this paper (Winning UNO with reinforcement learning). In that regard, we believe a two-layer connected neural network should be deep enough to report good results for either of the algorithms we used. It is worth noting that the hyperparameters they use give a lot of importance to exploration throughout the training, even with decay ($\\epsilon =  0.95, \\kappa = 0.995$ with decay every 10th of the way still results in a very explorative behavior: $\\epsilon \\times \\kappa^{10} \\approx 0.90$???). The discount rate is high ($\\gamma = 0.95$) considering winning rapidly or in a long time does not matter that much in the end, and the learning rate is $\\alpha=0.0001$.\n",
    "\n",
    "[Not sure about all that] The limited successes of some of our agents might stem from this aversion of some of the algorithms to exploration. For instance, REINFORCE does not give way to exploration. As the state space is really large, the policy might focus too much on specific episodes and states which have repeated throughout the training???\n",
    "\n",
    "#### Multiple players\n",
    "\n",
    "We have pondered over adding another random agent to solidify the training of our agents. Although the drawbacks of such a solution seem obvious (training will take longer - why should it be any different from 1v1 in terms of policy), adding players might result in more variance in the states covered throughout the episodes as there is more interaction between all players: in the end, this could reflect a better exploration throughout the training eventually reporting better results\n",
    "\n",
    "#### Two more base agents\n",
    "\n",
    "Some basic strategies come to mind for other baseline agents: one, which is widely played, is to play a card of the same value whenever it is possible, whatever the color is(\"value\"-strategy), and an other one could be to play the target color whenever possible (\"color\"-strategy). The value-strategy usually pans out better in the end because there is ...\n",
    "\n",
    "We havent added the \"saying UNO\" part but the obvious modeling is adding a Bernoulli variable when left with one card only: as this would be symmetric for all players, it wouldn't change the average reward, and would just take longer training cause of the variance added.\n",
    "\n",
    "#### State representation\n",
    "\n",
    "There are a lot of possible state representations of UNO. Choosing a good representation lies in reducing the complexity of the training as much as possible while still reflecting the dynamics of the game. We have thought about adding randomness to colors and values (as in, for example, every standard card could have a 1/4 chance to be of a given color every time) to get rid of these dimensions, and then building this agent over a baseline agent that understands these colors and values matchings. But it seemed too distant from the actual game, and maybe not worth it.\n",
    "\n",
    "#### Game is difficult\n",
    "\n",
    "Despite these modifications, Uno is a very stochastic game, and agents struggle to win over 60% of their games against random agents. This is very frustrating considering UNO does not seem to be that difficult of a game to play in the first place.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
