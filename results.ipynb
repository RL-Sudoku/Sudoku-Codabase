{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"./\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### State, Action & Reward Representation\n",
    "\n",
    "- We only consider 1v1 UNO\n",
    "\n",
    "- 2 states representations: naive representation for tabular methods (where you count all possibilities) results in way too large a state space to run tabular methods (if you do not even account for values this results in ~130M possible states). Needs reducing the state space through simplification --> 4 planes where each entry of a plane corresponds to a given card and 3 of those planes to having a given number of a specific card (you can either have 0, 1, or 2 of each card) ; the last plane is the target / open card.\n",
    "\n",
    "- Motivations of using function approximation: state space is way too big.\n",
    "\n",
    "- In total, there are 61 different actions a player can take, which are playing each of the 60 different cards and drawing a card from the deck.\n",
    "\n",
    "- The reward is defined as +1 for winning, −1 for losing, and 0 for all intermediate states. Also we assume the game is endless so all final rewards are either +1 or -1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture & Hyperparameter Selection\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "Add plot later\n",
    "\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "Add description later"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents & Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: Random Policy\n",
    "Random vs. Random: we observe that the advantage given by starting the game is not negligible (the random player that starts has an average reward of .02 if I remember). This is not surprising though: in a game where the goal is to let go of your cards as fast as possible, getting rid of cards in a first place will result in higher rewrad in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: 0.0084\n",
      "RANDOM Agent Average Reward: -0.0084\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 5042 games (RANDOM Agent win rate: 50.42%)\n",
      "RANDOM Agent wins 4958 games (RANDOM Agent win rate: 49.58%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0084, -0.0084)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tests.eval import test_trained_agents\n",
    "from uno.agents.random_agent import RandomAgent\n",
    "random_agent = RandomAgent(61)\n",
    "test_trained_agents(random_agent, random_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm I: REINFORCE\n",
    "\n",
    "TODO: Change this plot later\n",
    "\n",
    "TODO: Add description\n",
    "<!-- Average Rewards\n",
    "\n",
    "------------------------------------------------------------\n",
    "Reinforce Agent Average Reward: 0.0038\n",
    "RANDOM Agent Average Reward: -0.0038\n",
    "\n",
    "------------------------------------------------------------\n",
    "Total Number of Games: 10000\n",
    "Reinforce Agent wins 5019 games (Reinforce Agent win rate: 50.19%)\n",
    "RANDOM Agent wins 4981 games (RANDOM Agent win rate: 49.81%)\n",
    "Draws 0 games (Draw rate: 0.0%) -->\n",
    "\n",
    "<!-- Very inconclusive -->\n",
    "![avg_reward_reinforce.png](checkpoint/REINFORCE/avg_reward_reinforce.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.reinforce_agent import ReinforceAgent\n",
    "reinforce_agent = CHECKPOINTS['REINFORCE Agent']\n",
    "rewards = test_trained_agents(random_agent, reinforce_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = test_trained_agents(reinforce_agent, random_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm II: Monte Carlo On Policy Approximation\n",
    "\n",
    "TODO: Add description\n",
    "\n",
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/MC/mc-agent-[200000]-[0.0001]-[0.95]-[0.95]-[first].png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/MC/mc-agent-[200000]-[0.0001]-[0.95]-[0.95]-[first].png\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: -0.0336\n",
      "MC Agent Average Reward: 0.0336\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 4832 games (RANDOM Agent win rate: 48.32%)\n",
      "MC Agent wins 5168 games (MC Agent win rate: 51.68%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n",
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "MC Agent Average Reward: 0.041\n",
      "RANDOM Agent Average Reward: -0.041\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "MC Agent wins 5205 games (MC Agent win rate: 52.05%)\n",
      "RANDOM Agent wins 4795 games (RANDOM Agent win rate: 47.95%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.041, -0.041)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.mc_agent import MCAgent\n",
    "mc_agent = CHECKPOINTS['MC Agent']\n",
    "rewards = test_trained_agents(random_agent, mc_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = test_trained_agents(mc_agent, random_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm III: Double Q-Learning\n",
    "\n",
    "TODO: Add more descriptions\n",
    "\n",
    "#### Double-Q Network\n",
    "\n",
    "The Double Deep Q-Network (Double DQN) is an extension of the standard Deep Q-Network (DQN) that aims to address the overestimation bias often found in Q-learning algorithms. In traditional DQN, the same network is used to both select and evaluate the best action, which can lead to an overoptimistic estimation of action values. To mitigate this issue, the Double DQN employs two separate networks: one for action selection and another for action evaluation. During each iteration, the agent plays n games using ε-greedy exploration and generates a set of trajectories (s, a, r, s'). The Double DQN update rule then utilizes both networks by selecting the action with the highest Q-value from the first network and evaluating that action using the second network. This decouples the action selection and evaluation processes, reducing the overestimation bias and improving the stability of learning.\n",
    "\n",
    "dqn agent was trained with a set of different parameters:\n",
    "Number of episodes: 100k, 80k, 50k,\n",
    "decaying epsilon from 0.95 to 0.01 with decaying factor 0.95, updating every 1000 episodes.\n",
    "Constant epsilon: 0.05,0.5,0.8.\n",
    "\n",
    "\n",
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/DQN/dqn-agent-[200000]-[0.0001]-[no decay_0.08]-[0.95]-[first].png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/DQN/dqn-agent-[200000]-[0.0001]-[no decay_0.08]-[0.95]-[second].png\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.dqn_agent import DQNAgent\n",
    "dqn_agent = CHECKPOINTS['DQN Agent']\n",
    "rewards = test_trained_agents(dqn_agent, random_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = test_trained_agents(random_agent, dqn_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm IV: SARSA\n",
    "\n",
    "TODO: Add description\n",
    "\n",
    "\n",
    "\n",
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/SARSA/sarsa-agent-[200000]-[0.0001]-[0.95]-[0.95]-[first]-[0].png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/SARSA/sarsa-agent-[200000]-[0.0001]-[0.95]-[0.95]-[second]-[0].png\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.sarsa_agent import SARSAAgent\n",
    "sarsa_agent = CHECKPOINTS['SARSA Agent']\n",
    "rewards = test_trained_agents(random_agent, sarsa_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = test_trained_agents(sarsa_agent, random_agent, 10000, True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contests\n",
    "\n",
    "In this section, we play against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Agent</th>\n",
       "      <th>SARSA Agent</th>\n",
       "      <th>MC Agent</th>\n",
       "      <th>REINFORCE Agent</th>\n",
       "      <th>DQN Agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Agent</th>\n",
       "      <td>52.00%</td>\n",
       "      <td>52.00%</td>\n",
       "      <td>49.00%</td>\n",
       "      <td>58.00%</td>\n",
       "      <td>53.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SARSA Agent</th>\n",
       "      <td>64.00%</td>\n",
       "      <td>52.00%</td>\n",
       "      <td>48.00%</td>\n",
       "      <td>49.00%</td>\n",
       "      <td>50.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC Agent</th>\n",
       "      <td>43.00%</td>\n",
       "      <td>52.00%</td>\n",
       "      <td>49.00%</td>\n",
       "      <td>55.00%</td>\n",
       "      <td>47.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REINFORCE Agent</th>\n",
       "      <td>44.00%</td>\n",
       "      <td>51.00%</td>\n",
       "      <td>46.00%</td>\n",
       "      <td>45.00%</td>\n",
       "      <td>53.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DQN Agent</th>\n",
       "      <td>58.00%</td>\n",
       "      <td>53.00%</td>\n",
       "      <td>54.00%</td>\n",
       "      <td>46.00%</td>\n",
       "      <td>51.00%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Random Agent SARSA Agent MC Agent REINFORCE Agent DQN Agent\n",
       "Random Agent          52.00%      52.00%   49.00%          58.00%    53.00%\n",
       "SARSA Agent           64.00%      52.00%   48.00%          49.00%    50.00%\n",
       "MC Agent              43.00%      52.00%   49.00%          55.00%    47.00%\n",
       "REINFORCE Agent       44.00%      51.00%   46.00%          45.00%    53.00%\n",
       "DQN Agent             58.00%      53.00%   54.00%          46.00%    51.00%"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tests.tests import contests\n",
    "from tests.eval import *\n",
    "stats = contests(n=100)\n",
    "stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Limitation\n",
    "\n",
    "#### Hyperparameters & Architecture\n",
    "\n",
    "A majority of the agents we trained do not seem to have any edge over the random agent, which sounds very frustrating considering UNO does not look that complicated after all. A lot of parameters come into play regarding the performance of the agents we propose here however, namely the architecture of our NNs and the hyperparameters.\n",
    "\n",
    "Most of our fine-tuning is heavily inspired by this paper (Winning UNO with reinforcement learning). In that regard, we believe a two-layer connected neural network should be deep enough to report good results for either of the algorithms we used. It is worth noting that the hyperparameters they use give a lot of importance to exploration throughout the training, even with decay ($\\epsilon =  0.95, \\kappa = 0.995$ with decay every 10th of the way still results in a very explorative behavior: $\\epsilon \\times \\kappa^{10} \\approx 0.90$???). The discount rate is high ($\\gamma = 0.95$) considering winning rapidly or in a long time does not matter that much in the end, and the learning rate is $\\alpha=0.0001$.\n",
    "\n",
    "[Not sure about all that] The limited successes of some of our agents might stem from this aversion of some of the algorithms to exploration. For instance, REINFORCE does not give way to exploration. As the state space is really large, the policy might focus too much on specific episodes and states which have repeated throughout the training???\n",
    "\n",
    "#### Multiple players\n",
    "\n",
    "We have pondered over adding another random agent to solidify the training of our agents. Although the drawbacks of such a solution seem obvious (training will take longer - why should it be any different from 1v1 in terms of policy), adding players might result in more variance in the states covered throughout the episodes as there is more interaction between all players: in the end, this could reflect a better exploration throughout the training eventually reporting better results\n",
    "\n",
    "#### Two more base agents\n",
    "\n",
    "Some basic strategies come to mind for other baseline agents: one, which is widely played, is to play a card of the same value whenever it is possible, whatever the color is(\"value\"-strategy), and an other one could be to play the target color whenever possible (\"color\"-strategy). The value-strategy usually pans out better in the end because there is ...\n",
    "\n",
    "We havent added the \"saying UNO\" part but the obvious modeling is adding a Bernoulli variable when left with one card only: as this would be symmetric for all players, it wouldn't change the average reward, and would just take longer training cause of the variance added.\n",
    "\n",
    "#### State representation\n",
    "\n",
    "There are a lot of possible state representations of UNO. Choosing a good representation lies in reducing the complexity of the training as much as possible while still reflecting the dynamics of the game. We have thought about adding randomness to colors and values (as in, for example, every standard card could have a 1/4 chance to be of a given color every time) to get rid of these dimensions, and then building this agent over a baseline agent that understands these colors and values matchings. But it seemed too distant from the actual game, and maybe not worth it.\n",
    "\n",
    "#### Game is difficult\n",
    "\n",
    "Despite these modifications, Uno is a very stochastic game, and agents struggle to win over 60% of their games against random agents. This is very frustrating considering UNO does not seem to be that difficult of a game to play in the first place.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
