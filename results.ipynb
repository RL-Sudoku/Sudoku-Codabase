{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Application in UNO Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "sys.path.append(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For our project, we want to apply Reinforcement Learning techniques to the game of Uno. We will consider 1v1 UNO with two key assumptions: first we have an infinite number of decks, and we must finish the current deck before moving onto the next. Second, the game will always terminate, meaning there are no ties).\n",
    "\n",
    "### State ($\\mathcal{S}$)\n",
    "One naive representation for the Uno is utilizing tabular methods, which stores all possibiles states in a table. However, if we wanted to accurately model the game of Uno, the state space is too large to be able to efficiently run tabular methods. In our initial research, we determined that if do not account for the value of the card (and not the color), we would have approximately $~130M$ possible states. Therefore, we will reduce the state space through the following simplification:\n",
    "\n",
    "- Note: In a deck of Uno cards, there are 60 unique cards with 2 of each type.\n",
    "- $4$ planes where each entry of a plane corresponds to a given card and 3 of those planes to having a given number of a specific card (you can either have 0, 1, or 2 of each card); the last plane is the target / open card.\n",
    "- Each plane is of dimension $(4, 15)$, representing $4$ colors and $15$ possible card values. \n",
    "- The encoding is binary for all planes.\n",
    "\n",
    "- Motivations of using `function approximation`: state space is way too big.\n",
    "\n",
    "### Action ($\\mathcal{A}$)\n",
    "In total, there are $61$ different actions a player can take, to which $4 \\times 15 = 60$ of them are for playing different cards and $1$ action for drawing a card from the deck. However, only a few of them are legal actions depending on different states, which will be checked carefully in our game simulation and training.\n",
    "\n",
    "### Reward ($\\mathcal{R}$) \n",
    "The reward is defined as $+1$ for winning, $−1$ for losing, and $0$ for all intermediate states. Based on our assumptions, the game must terminate, so all final rewards are either $+1$ or $-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "In order to better model the state space, we chose a standard _two-layer fully connected_ NN as our approximation model. Specifically, we have tried out different combinations of the number of layers and hidden dimension, but we found that a hidden dimension of $512$ yields the best performance. Note in the diagram below that we have an input of $4 * 4* 15 = 240$ parameters for the state representation, and the output has $61$ parameters—one for each action.\n",
    "<!-- \n",
    "- `nn.Linear(4 x 4 x 15, 512)`\n",
    "- `nn.ReLU()`\n",
    "- `nn.Linear(512, 512)`\n",
    "- `nn.ReLU()`\n",
    "- `nn.Linear(512, 61)` -->\n",
    "<center>\n",
    "<img style=\"height: auto; width: 60%;\" class=\"img\" src=\"log/neural-network.png\" >\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 61]                   --\n",
       "├─Linear: 1-1                            [1, 512]                  123,392\n",
       "├─ReLU: 1-2                              [1, 512]                  --\n",
       "├─Linear: 1-3                            [1, 512]                  262,656\n",
       "├─ReLU: 1-4                              [1, 512]                  --\n",
       "├─Linear: 1-5                            [1, 61]                   31,293\n",
       "==========================================================================================\n",
       "Total params: 417,341\n",
       "Trainable params: 417,341\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 0.42\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 1.67\n",
       "Estimated Total Size (MB): 1.68\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model.backbone import Q\n",
    "from torchinfo import summary\n",
    "model = Q()\n",
    "summary(model.model, (1, 4 * 4 * 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Agent\n",
    "\n",
    "`Random Agent` acts by randomly choosing one action from available legal actions.\n",
    "\n",
    "By playing two `Random Agents` against each other, we observe that the advantage given by playing first is not negligible: the first player that has winning rate around $51\\%$. This is not surprising though: in a game where the goal is to let go of your cards as fast as possible, playing first will allow you to getting rid of cards first, and this will result in higher reward in the long run.\n",
    "\n",
    "While we do note that the increase of the winning rate is small, we also understand that UNO is a highly stochastic game, and thus the advantage of being the first player is not as significant as what we have seen in Tic-Tac-Toe. There is only a slight advantage of $2\\%$ in terms of winning rate. With this observation, we have choosen to train all of our algorithms as the second player as this is a relatively more difficult task and may result in better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:00<00:00, 165.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: 0.0192\n",
      "RANDOM Agent Average Reward: -0.0192\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 5096 games (RANDOM Agent win rate: 50.96%)\n",
      "RANDOM Agent wins 4904 games (RANDOM Agent win rate: 49.04%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "from tests.eval import test_trained_agents\n",
    "from uno.agents.random_agent import RandomAgent\n",
    "\n",
    "random_agent = RandomAgent(61)\n",
    "rewards = test_trained_agents(random_agent, random_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection\n",
    "\n",
    "One key aspect of our model's performance is the initialization of the hyperparameters. In order to find the model that results in the best trained agent, we experimented with $3 * 2 * (2 + 2) = 24$ sets of parameters for each algorithm for $50000$ episodes. For the case where we set $\\epsilon=0.95$, we exponentially decayed it every $100$ episodes by a factor of $\\kappa$.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Hyperparameters   |      Short Description     |  Candidate Values List |\n",
    "|:-----------------:|:-------------------:|:----------------------:|\n",
    "| $\\eta$ |  Learning Rate | $[10^{-2}, 10^{-3}, 10^{-4}]$ |\n",
    "| $\\gamma$ |  Discount Factor | $[0.95, 0.99]$ |\n",
    "| $\\epsilon$ |    Exploration vs. Exploitation  |   $[0.05, 0.10,0.95]$ |\n",
    "| $\\kappa$ (optional) | Control Decay of $\\epsilon$ (required when $\\epsilon = 0.95$) |    $[0.90, 0.95]$ |\n",
    "\n",
    "</center>\n",
    "\n",
    "To clarify, the parameters such as the number of training episodes and frequency of decaying $\\epsilon$ can also be regarded as hyperparameters. However, through numerous experiments, we finally decided to train every model for $200K$ episodes, and if we are using $\\epsilon=0.95$, it should be decayed every $2000$ episodes.\n",
    "\n",
    "**Note.** Unfortunately, due to high randomness, training $200K$ episodes of UNO takes more than 8 hours on our laptop, so we are not able to train an agent for longer due to limited resources.\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "Every $1000$ episodes during training, we evaluate our training by simulating $10K$ games between the `Random Agent` and our learned model. This allows us to see if there is an upward trend in the average reward and winning rate. In addition, we base the hyperparameter selection on this metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents & Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm I: Monte Carlo On Policy Approximation\n",
    "\n",
    "We trained our `MC Agent` for $200K$ episodes with $\\eta=10^{-4}$, $\\gamma=0.95$, $\\epsilon=0.95$, and decay rate $\\kappa=0.95$. In general, we did not see big performance gain for the MC agent, although there is about $1.5\\%  - 2\\%$ of improvement. Relative to one of the references we found in our initial research, this is actually on par with their results. We suspect that the lack of a significant improvement may be rooted in our hyperparameter selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/MC/mc-agent-[200000]-[0.0001]-[0.95]-[0.95]-[first].png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/MC/mc-agent-[200000]-[0.0001]-[0.95]-[0.95]-[second].png\" /></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "MC Agent Average Reward: 0.0384\n",
      "RANDOM Agent Average Reward: -0.0384\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "MC Agent wins 5192 games (MC Agent win rate: 51.92%)\n",
      "RANDOM Agent wins 4808 games (RANDOM Agent win rate: 48.08%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.mc_agent import MCAgent\n",
    "mc_agent = CHECKPOINTS['MC Agent']\n",
    "rewards = test_trained_agents(mc_agent, random_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: -0.0258\n",
      "MC Agent Average Reward: 0.0258\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 4871 games (RANDOM Agent win rate: 48.71%)\n",
      "MC Agent wins 5129 games (MC Agent win rate: 51.29%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "rewards = test_trained_agents(random_agent, mc_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm II: SARSA\n",
    "\n",
    "Next, we trained our `SARSA Agent` for $200K$ episodes with $\\eta=10^{-4}$, $\\gamma=0.95$, $\\epsilon=0.95$, and decay rate $\\kappa=0.95$. Surprisingly, this is our best trained again in terms of the winning rate. In general, the `SARSA Agent` achieved above a $56\\%$ winning rate in $10000$ simluations. Relative to the stochasticity of Uno, this rate is considered to be a drastic increase in playing performance.\n",
    "\n",
    "Furthermore, in one of our references, they foudn that their winning rate of their `SARSA Agent` is roughly $52\\%$, insinuating that there is validity and a certain amount of accuracy in our state representation and training strategy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/SARSA/sarsa-agent-[200000]-[0.0001]-[0.95]-[0.95]-[first]-[0].png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/SARSA/sarsa-agent-[200000]-[0.0001]-[0.95]-[0.95]-[second]-[0].png\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:42<00:00, 97.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: -0.1204\n",
      "SARSA Agent Average Reward: 0.1204\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 4398 games (RANDOM Agent win rate: 43.98%)\n",
      "SARSA Agent wins 5602 games (SARSA Agent win rate: 56.02%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "from uno.agents.sarsa_agent import SARSAAgent\n",
    "sarsa_agent = CHECKPOINTS['SARSA Agent']\n",
    "rewards = test_trained_agents(random_agent, sarsa_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:26<00:00, 115.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "SARSA Agent Average Reward: 0.141\n",
      "RANDOM Agent Average Reward: -0.141\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "SARSA Agent wins 5705 games (SARSA Agent win rate: 57.05%)\n",
      "RANDOM Agent wins 4295 games (RANDOM Agent win rate: 42.95%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "rewards = test_trained_agents(sarsa_agent, random_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm III: REINFORCE\n",
    "\n",
    "We trained our `REINFORCE Agent` for $70K$ episodes with $\\gamma=0.95$ and $\\eta=10^{-4}$. Since the REINFORCE algorithm is based on a different methodology and we found that a `REINFORCE Agent` able to win only about $51\\%$ of the time, we suspect that REINFORCE may not be the best RL algorithm for Uno.\n",
    "\n",
    "<!-- However, the results are not promising. In this algorithm, we forgot to use $\\epsilon$-greedy so this might explain why `REINFORCE Agent` is not working as desired. We are training a new one with $\\epsilon$-greedy strategy now and hopefully it will give us better results. -->\n",
    "<!-- Average Rewards\n",
    "\n",
    "------------------------------------------------------------\n",
    "Reinforce Agent Average Reward: 0.0038\n",
    "RANDOM Agent Average Reward: -0.0038\n",
    "\n",
    "------------------------------------------------------------\n",
    "Total Number of Games: 10000\n",
    "Reinforce Agent wins 5019 games (Reinforce Agent win rate: 50.19%)\n",
    "RANDOM Agent wins 4981 games (RANDOM Agent win rate: 49.81%)\n",
    "Draws 0 games (Draw rate: 0.0%) -->\n",
    "\n",
    "<!-- Very inconclusive -->\n",
    "<!-- ![avg_reward_reinforce.png](checkpoint/REINFORCE/avg_reward_reinforce.png) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/REINFORCE/reinforce_plays_first.png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/REINFORCE/reinforce_plays_second.png\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: -0.0212\n",
      "REINFORCE Agent Average Reward: 0.0212\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 4894 games (RANDOM Agent win rate: 48.94%)\n",
      "REINFORCE Agent wins 5106 games (REINFORCE Agent win rate: 51.06%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.reinforce_agent import ReinforceAgent\n",
    "reinforce_agent = CHECKPOINTS['REINFORCE Agent']\n",
    "rewards = test_trained_agents(random_agent, reinforce_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "REINFORCE Agent Average Reward: 0.0132\n",
      "RANDOM Agent Average Reward: -0.0132\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "REINFORCE Agent wins 5066 games (REINFORCE Agent win rate: 50.66%)\n",
      "RANDOM Agent wins 4934 games (RANDOM Agent win rate: 49.34%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "rewards = test_trained_agents(reinforce_agent, random_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm IV: Q-Learning\n",
    "\n",
    "#### Double Deep Q_Network\n",
    "\n",
    "The Double Deep Q-Network (Double DQN) is an extension of the standard Deep Q-Network (DQN) that aims to reduce the overestimation bias often found in Q-learning algorithms. In traditional DQN, the same network is used to both select and evaluate the best policy, leading to an overoptimistic estimation of action values. To mitigate this issue, the Double DQN exploits two separate networks: one for action selection and another for action evaluation. During each iteration, the agent plays n games using ε-greedy exploration and generates a set of trajectories (S, A, R, S'). The Double DQN update rule then utilizes both networks by selecting the action with the highest Q-value from the first network and evaluating that action using the second network. This decouples the action selection and evaluation processes, thus reducing the overestimation bias and improving the stability of learning.\n",
    "\n",
    "The `DQN Agent` had the following hyperparameters: $\\eta=10^{-4}$, $\\gamma=0.95$, $\\epsilon=0.95$. Overall, we notice a slight increase to its winning rate at $51\\%$, but nothing on the scale of the `SARSA Agent.` DQN agent was trained with a set of different parameters:\n",
    "Architecture: one hidden layer with size $128$, we are able to achieve 200K training in a reasonable time.\n",
    "\n",
    "\n",
    "\n",
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/DQN/dqn-agent-[200000]-[0.0001]-[no decay_0.08]-[0.95]-[first].png\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/DQN/dqn-agent-[200000]-[0.0001]-[no decay_0.08]-[0.95]-[second].png\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "DQN Agent Average Reward: 0.029\n",
      "RANDOM Agent Average Reward: -0.029\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "DQN Agent wins 5145 games (DQN Agent win rate: 51.45%)\n",
      "RANDOM Agent wins 4855 games (RANDOM Agent win rate: 48.55%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "from tests.tests import *\n",
    "from uno.agents.dqn_agent import DQNAgent\n",
    "dqn_agent = CHECKPOINTS['DQN Agent']\n",
    "rewards = test_trained_agents(dqn_agent, random_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Average Rewards\n",
      "------------------------------------------------------------\n",
      "RANDOM Agent Average Reward: -0.0148\n",
      "DQN Agent Average Reward: 0.0148\n",
      "\n",
      "------------------------------------------------------------\n",
      "Total Number of Games: 10000\n",
      "RANDOM Agent wins 4926 games (RANDOM Agent win rate: 49.26%)\n",
      "DQN Agent wins 5074 games (DQN Agent win rate: 50.74%)\n",
      "Draws 0 games (Draw rate: 0.0%)\n"
     ]
    }
   ],
   "source": [
    "rewards = test_trained_agents(random_agent, dqn_agent, 10000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However the results is not as expected.\n",
    "So we expanded our architecture to two hidden layer with size 128 and 512\n",
    "With a 70K episode training, we are able to get the below graph.(new graph) \n",
    "\n",
    "With a slightly better average reward and average wining rate, and mainly above the 50% line\n",
    "\n",
    "<div class='container'>\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/DQN/1.jpeg\" />\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "<img style=\"height: auto; width: 45%;\" class=\"img\" src=\"log/DQN/2.jpeg\" /></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contests\n",
    "\n",
    "After training each agent, we played them against each other. Due to the overwhelming increase of winning rate for the `SARSA Agent`, we see that it wins the contest, both as the first player and as the second player. For clarity, the percentages listed in the table below represent the winning rate of the row player, given that the row player goes first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 190.03it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 97.87it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 100.01it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 96.81it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 91.85it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 103.09it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 99.53it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 88.47it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 85.55it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 85.49it/s]\n",
      "100%|██████████| 1000/1000 [00:09<00:00, 104.58it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 83.19it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 77.63it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 68.89it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 82.05it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 98.44it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 86.61it/s]\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 72.93it/s]\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 71.82it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 69.29it/s]\n",
      "100%|██████████| 1000/1000 [00:10<00:00, 99.45it/s]\n",
      "100%|██████████| 1000/1000 [00:11<00:00, 85.48it/s]\n",
      "100%|██████████| 1000/1000 [00:15<00:00, 64.58it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 70.37it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 70.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Agent</th>\n",
       "      <th>SARSA Agent</th>\n",
       "      <th>MC Agent</th>\n",
       "      <th>REINFORCE Agent</th>\n",
       "      <th>DQN Agent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Random Agent</th>\n",
       "      <td>50.70%</td>\n",
       "      <td>45.30%</td>\n",
       "      <td>48.60%</td>\n",
       "      <td>51.30%</td>\n",
       "      <td>51.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SARSA Agent</th>\n",
       "      <td>57.70%</td>\n",
       "      <td>52.40%</td>\n",
       "      <td>56.30%</td>\n",
       "      <td>56.00%</td>\n",
       "      <td>53.80%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MC Agent</th>\n",
       "      <td>50.20%</td>\n",
       "      <td>45.80%</td>\n",
       "      <td>48.20%</td>\n",
       "      <td>53.00%</td>\n",
       "      <td>50.70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REINFORCE Agent</th>\n",
       "      <td>48.90%</td>\n",
       "      <td>42.30%</td>\n",
       "      <td>48.80%</td>\n",
       "      <td>51.10%</td>\n",
       "      <td>47.70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DQN Agent</th>\n",
       "      <td>52.20%</td>\n",
       "      <td>43.20%</td>\n",
       "      <td>52.10%</td>\n",
       "      <td>51.20%</td>\n",
       "      <td>47.90%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Random Agent SARSA Agent MC Agent REINFORCE Agent DQN Agent\n",
       "Random Agent          50.70%      45.30%   48.60%          51.30%    51.00%\n",
       "SARSA Agent           57.70%      52.40%   56.30%          56.00%    53.80%\n",
       "MC Agent              50.20%      45.80%   48.20%          53.00%    50.70%\n",
       "REINFORCE Agent       48.90%      42.30%   48.80%          51.10%    47.70%\n",
       "DQN Agent             52.20%      43.20%   52.10%          51.20%    47.90%"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2023)\n",
    "from tests.tests import contests\n",
    "from tests.eval import *\n",
    "stats = contests(n=1000)\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion & Limitation\n",
    "\n",
    "### More on Hyperparameters & Architecture\n",
    "\n",
    "A majority of the agents we trained do not seem to have any edge over the random agent, which may seem rather counterintuitive, especially when we realize that UNO is not complicated game. A lot of parameters affect the performance of the agents we propose here, namely the architecture of our NNs and the hyperparameters.\n",
    "\n",
    "Most of our fine-tuning is heavily inspired by this paper (Winning UNO with Reinforcement Learning). In this regard, we believe a two-layer connected neural network should be deep enough to report good results for all of the algorithms we used. It is worth noting that the hyperparameters they use give a lot of weight to initial exploration throughout the training even with their decay factor. For example, setting $\\epsilon =  0.95$ and $\\kappa = 0.995$ with decay occurring 10th of the way still results in a very explorative behavior— $\\epsilon \\times \\kappa^{10} \\approx 0.90$). We deem the discount rate is to be ratherhigh ($\\gamma = 0.95$), considering winning rapidly or in a long time does not matter that much in the end, and the learning rate is $\\alpha=0.0001$.\n",
    "\n",
    "The limited successes of some of our agents might stem from this aversion of some of the algorithms to exploration. For instance, REINFORCE does not give way to exploration. As the state space is really large, the policy might focus too much on specific episodes and states which have repeated throughout the training.\n",
    "\n",
    "### Multiple players\n",
    "\n",
    "We have pondered over adding another random agent to solidify the training of our agents. Although the drawbacks of such a solution seem obvious (training will take inevitably longer - why should it be any different from 1v1 in terms of policy), adding players might result in more variance in the states covered throughout the episodes as there is more interaction between all players. In the end, this could reflect a better exploration throughout the training and eventually report better performance results.\n",
    "\n",
    "#### Two more base agents\n",
    "\n",
    "Some basic strategies come to mind for other baseline agents: one, which is widely played, is to play a card of the same value whenever it is possible, regardless the color is (denoted as the \"value\"-strategy), and an other one could be to play the target color whenever possible (denoted as the \"color\"-strategy). The value-strategy usually pans out better in the end because there is there are more cards to play based on color (roughly 1/4 of the deck) versus to play based on value (roughly 1/15).\n",
    "\n",
    "We havent added the \"saying UNO\" part but the obvious modeling is adding a Bernoulli variable when left with one card only: as this would be symmetric for all players, it wouldn't change the average reward, and would just take longer training cause of the variance added.\n",
    "\n",
    "### State representation\n",
    "\n",
    "There are a lot of possible state representations of UNO. Choosing a good representation lies in reducing the complexity of the training as much as possible while still reflecting the dynamics of the game. We have thought about adding randomness to colors and values (as in, for example, every standard card could have a 1/4 chance to be of a given color every time) to get rid of these dimensions, and then building this agent over a baseline agent that understands these colors and values matchings. But it seemed too distant from the actual game, and maybe not reflect how one would go about playing Uno.\n",
    "\n",
    "### Game is difficult\n",
    "\n",
    "Despite these modifications, Uno is a very stochastic game, and agents struggle to win over 60% of their games against random agents. If we had unlimited resources in terms of CPU and time, we might be able to achieve better results by letting each agent train for more than 200K games. Nonetheless, we do see that there are still nonneglibible improvements by using the RL techniques we've learned in class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
